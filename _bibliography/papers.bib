---
---

@misc{li2024upsampleupweightbalancedtraining,
      title={Upsample or Upweight? Balanced Training on Heavily Imbalanced Datasets}, 
      author={Tianjian Li and Haoran Xu and Weiting Tan and Dongwei Jiang and Kenton Murray and Daniel Khashabi},
      year={2024},
      eprint={2410.04579},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.04579}, 
      abstract={Data availability across domains often follows a long-tail distribution: a few domains have abundant data, while most face data scarcity. This imbalance poses challenges in training language models uniformly across all domains. In our study, we focus on multilingual settings, where data sizes vary significantly between high- and low-resource languages. Common strategies to address this include upsampling low-resource languages (Temperature Sampling) or upweighting their loss (Scalarization). Although often considered equivalent, this assumption has not been proven, which motivates our study. Through both theoretical and empirical analysis, we identify the conditions under which these approaches are equivalent and when they diverge. Specifically, we demonstrate that these two methods are equivalent under full gradient descent, but this equivalence breaks down with stochastic gradient descent. Empirically, we observe that Temperature Sampling converges more quickly but is prone to overfitting. We argue that this faster convergence is likely due to the lower variance in gradient estimations, as shown theoretically. Based on these insights, we propose Cooldown, a strategy that reduces sampling temperature during training, accelerating convergence without overfitting to low-resource languages. Our method is competitive with existing data re-weighting and offers computational efficiency.},
      abbr={preprint}
}

@misc{tan2024ssralignmentawaremodalityconnector,
      title={SSR: Alignment-Aware Modality Connector for Speech Language Models}, 
      author={Weiting Tan and Hirofumi Inaguma and Ning Dong and Paden Tomasello and Xutai Ma},
      year={2024},
      eprint={2410.00168},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.00168}, 
      abstract={Fusing speech into pre-trained language model (SpeechLM) usually suffers from inefficient encoding of long-form speech and catastrophic forgetting of pre-trained text modality. We propose SSR-Connector (Segmented Speech Representation Connector) for better modality fusion. Leveraging speech-text alignments, our approach segments and compresses speech features to match the granularity of text embeddings. Additionally, we introduce a two-stage training pipeline that includes the distillation and fine-tuning phases to mitigate catastrophic forgetting. SSR-Connector outperforms existing mechanism for speech-text modality fusion, consistently achieving better speech understanding (e.g., +10 accuracy on StoryCloze and +20 on Speech-MMLU) while preserving pre-trained text ability.},
      selected={true},
      abbr={preprint}
}

@inproceedings{romney-robinson-etal-2024-jhu,
    title = "{JHU} {IWSLT} 2024 Dialectal and Low-resource System Description",
    author = "Romney Robinson, Nathaniel  and
      Sun, Kaiser  and
      Xiao, Cihan  and
      Bafna, Niyati  and
      Tan, Weiting  and
      Xu, Haoran  and
      Li Xinyuan, Henry  and
      Kejriwal, Ankur  and
      Khudanpur, Sanjeev  and
      Murray, Kenton  and
      McNamee, Paul",
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand (in-person and online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.iwslt-1.19",
    pages = "140--153",
    abstract = "Johns Hopkins University (JHU) submitted systems for all eight language pairs in the 2024 Low-Resource Language Track. The main effort of this work revolves around fine-tuning large and publicly available models in three proposed systems: i) end-to-end speech translation (ST) fine-tuning of Seamless4MT v2; ii) ST fine-tuning of Whisper; iii) a cascaded system involving automatic speech recognition with fine-tuned Whisper and machine translation with NLLB. On top of systems above, we conduct a comparative analysis on different training paradigms, such as intra-distillation for NLLB as well as joint training and curriculum learning for SeamlessM4T v2. Our results show that the best-performing approach differs by language pairs, but that i) fine-tuned SeamlessM4T v2 tends to perform best for source languages on which it was pre-trained, ii) multi-task training helps Whisper fine-tuning, iii) cascaded systems with Whisper and NLLB tend to outperform Whisper alone, and iv) intra-distillation helps NLLB fine-tuning.",
    abbr={IWSLT}
}


@misc{lu2024takestwoseamlessnessreward,
      title={It Takes Two: On the Seamlessness between Reward and Policy Model in RLHF}, 
      author={Taiming Lu and Lingfeng Shen and Xinyu Yang and Weiting Tan and Beidi Chen and Huaxiu Yao},
      year={2024},
      eprint={2406.07971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.07971},
      abstract={Reinforcement Learning from Human Feedback (RLHF) involves training policy models (PMs) and reward models (RMs) to align language models with human preferences. Instead of focusing solely on PMs and RMs independently, we propose to examine their interactions during fine-tuning, introducing the concept of seamlessness. Our study starts with observing the saturation phenomenon, where continual improvements in RM and PM do not translate into RLHF progress. Our analysis shows that RMs fail to assign proper scores to PM responses, resulting in a 35% mismatch rate with human preferences, highlighting a significant discrepancy between PM and RM. To measure seamlessness between PM and RM without human effort, we propose an automatic metric, SEAM. SEAM quantifies the discrepancies between PM and RM judgments induced by data samples. We validate the effectiveness of SEAM in data selection and model augmentation. Our experiments demonstrate that (1) using SEAM-filtered data for RL training improves RLHF performance by 4.5%, and (2) SEAM-guided model augmentation results in a 4% performance improvement over standard augmentation methods.},
      abbr={ICML Workshop}
}

@misc{tan2024diffnormselfsupervisednormalizationnonautoregressive,
      title={DiffNorm: Self-Supervised Normalization for Non-autoregressive Speech-to-speech Translation}, 
      author={Weiting Tan and Jingyu Zhang and Lingfeng Shen and Daniel Khashabi and Philipp Koehn},
      year={2024},
      eprint={2405.13274},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.13274}, 
      abstract={Non-autoregressive Transformers (NATs) are recently applied in direct speech-to-speech translation systems, which convert speech across different languages without intermediate text data. Although NATs generate high-quality outputs and offer faster inference than autoregressive models, they tend to produce incoherent and repetitive results due to complex data distribution (e.g., acoustic and linguistic variations in speech). In this work, we introduce DiffNorm, a diffusion-based normalization strategy that simplifies data distributions for training NAT models. After training with a self-supervised noise estimation objective, DiffNorm constructs normalized target data by denoising synthetically corrupted speech features. Additionally, we propose to regularize NATs with classifier-free guidance, improving model robustness and translation quality by randomly dropping out source information during training. Our strategies result in a notable improvement of about +7 ASR-BLEU for English-Spanish (En-Es) and +2 ASR-BLEU for English-French (En-Fr) translations on the CVSS benchmark, while attaining over 14x speedup for En-Es and 5x speedup for En-Fr translations compared to autoregressive baselines.},
      selected={true},
      abbr={NeurIPS}
}

@misc{tan2024streamingsequencetransductiondynamic,
      title={Streaming Sequence Transduction through Dynamic Compression}, 
      author={Weiting Tan and Yunmo Chen and Tongfei Chen and Guanghui Qin and Haoran Xu and Heidi C. Zhang and Benjamin Van Durme and Philipp Koehn},
      year={2024},
      eprint={2402.01172},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.01172}, 
      abstract={We introduce STAR (Stream Transduction with Anchor Representations), a novel Transformer-based model designed for efficient sequence-to-sequence transduction over streams. STAR dynamically segments input streams to create compressed anchor representations, achieving nearly lossless compression (12x) in Automatic Speech Recognition (ASR) and outperforming existing methods. Moreover, STAR demonstrates superior segmentation and latency-quality trade-offs in simultaneous speech-to-text tasks, optimizing latency, memory footprint, and quality},
      selected={true},
      abbr={preprint},
}


@inproceedings{pmlr-v235-xu24t,
  title = {Contrastive Preference Optimization: Pushing the Boundaries of {LLM} Performance in Machine Translation},
  author = {Xu, Haoran and Sharaf, Amr and Chen, Yunmo and Tan, Weiting and Shen, Lingfeng and Van Durme, Benjamin and Murray, Kenton and Kim, Young Jin},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  pages = {55204--55224},
  year = {2024},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = {235},
  series = {Proceedings of Machine Learning Research},
  month = {21--27 Jul},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v235/xu24t.html},
  abstract={Moderate-sized large language models (LLMs) -- those with 7B or 13B parameters -- exhibit promising machine translation (MT) performance. However, even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4. In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to SFT which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA models with only 22K parallel sentences and 12M parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets.},
  selected={true},
  abbr={ICML}
}

@inproceedings{tan-etal-2024-narrowing,
    title = "Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles",
    author = "Tan, Weiting  and
      Xu, Haoran  and
      Shen, Lingfeng  and
      Li, Shuyue Stella  and
      Murray, Kenton  and
      Koehn, Philipp  and
      Van Durme, Benjamin  and
      Chen, Yunmo",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.33",
    doi = "10.18653/v1/2024.findings-naacl.33",
    pages = "490--502",
    abstract = "Large language models trained primarily in a monolingual setting have demonstrated their ability to generalize to machine translation using zero- and few-shot examples with in-context learning. However, even though zero-shot translations are relatively good, there remains a discernible gap comparing their performance with the few-shot setting. In this paper, we investigate the factors contributing to this gap and find that this gap can largely be closed (for about 70{\%}) by matching the writing styles of the target corpus. Additionally, we explore potential approaches to enhance zero-shot baselines without the need for parallel demonstration examples, providing valuable insights into how these methods contribute to improving translation metrics.",
    selected={true},
    abbr={NAACL}
}

@inproceedings{shen-etal-2024-language,
    title = "The Language Barrier: Dissecting Safety Challenges of {LLM}s in Multilingual Contexts",
    author = "Shen, Lingfeng  and
      Tan, Weiting  and
      Chen, Sihao  and
      Chen, Yunmo  and
      Zhang, Jingyu  and
      Xu, Haoran  and
      Zheng, Boyuan  and
      Koehn, Philipp  and
      Khashabi, Daniel",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.156",
    pages = "2668--2680",
    abstract = "As the influence of large language models (LLMs) spans across global communities, their safety challenges in multilingual settings become paramount for alignment research. This paper examines the variations in safety challenges faced by LLMs across different languages and discusses approaches to alleviating such concerns. By comparing how state-of-the-art LLMs respond to the same set of malicious prompts written in higher- vs. lower-resource languages,we observe that (1) LLMs tend to generate unsafe responses much more often when a malicious prompt is written in a lower-resource language, and (2) LLMs tend to generate more irrelevant responses to malicious prompts in lower-resource languages. To understand where the discrepancy can be attributed, we study the effect of instruction tuning with reinforcement learning from human feedback (RLHF) or supervised finetuning (SFT) on the HH-RLHF dataset. Surprisingly, while training with high-resource languages improves model alignment, training in lower-resource languages yields minimal improvement. This suggests that the bottleneck of cross-lingual alignment is rooted in the pretraining stage. Our findings highlight the challenges in cross-lingual LLM safety, and we hope they inform future research in this direction.",
    selected={true},
    abbr={ACL}
}




@inproceedings{tan-etal-2023-multilingual,
    title = "Multilingual Representation Distillation with Contrastive Learning",
    author = "Tan, Weiting  and
      Heffernan, Kevin  and
      Schwenk, Holger  and
      Koehn, Philipp",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.108",
    doi = "10.18653/v1/2023.eacl-main.108",
    pages = "1477--1490",
    abstract = "Multilingual sentence representations from large models encode semantic information from two or more languages and can be used for different cross-lingual information retrieval and matching tasks. In this paper, we integrate contrastive learning into multilingual representation distillation and use it for quality estimation of parallel sentences (i.e., find semantically similar sentences that can be used as translations of each other). We validate our approach with multilingual similarity search and corpus filtering tasks. Experiments across different low-resource languages show that our method greatly outperforms previous sentence encoders such as LASER, LASER3, and LaBSE.",
    selected={true},
    abbr={EACL}
}


@inproceedings{shen-etal-2023-flatness,
    title = "Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency",
    author = "Shen, Lingfeng  and
      Tan, Weiting  and
      Zheng, Boyuan  and
      Khashabi, Daniel",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.523",
    doi = "10.18653/v1/2023.findings-emnlp.523",
    pages = "7795--7817",
    abstract = "With growing capabilities of large language models, prompting them has become the dominant way to access them. This has motivated the development of strategies for automatically selecting effective language prompts. In this paper, we introduce **pFlat** (prompt flatness), a new metric to quantify the expected utility of a language prompt. This metric is inspired by *flatness* regularization in statistical learning that quantifies the robustness of the model towards its parameter perturbations. We provide theoretical foundations for this metric and its relationship with other prompt selection metrics, providing a comprehensive understanding of existing methods. Empirically, we show that combining **pFlat** with existing metrics improves both performance and sample efficiency. Our metric outperforms the previous prompt selection metrics with an average increase of 10{\%} in Pearson correlation across 6 classification benchmarks, and the prompt selected by our metric gains 5{\%} higher accuracy than previous metrics across the benchmarks.",
    abbr={EMNLP},
    selected={true},
}

@inproceedings{xu-etal-2023-condensing,
    title = "Condensing Multilingual Knowledge with Lightweight Language-Specific Modules",
    author = "Xu, Haoran  and
      Tan, Weiting  and
      Li, Shuyue  and
      Chen, Yunmo  and
      Van Durme, Benjamin  and
      Koehn, Philipp  and
      Murray, Kenton",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.97",
    doi = "10.18653/v1/2023.emnlp-main.97",
    pages = "1575--1587",
    abstract = "Incorporating language-specific (LS) modules or Mixture-of-Experts (MoE) are proven methods to boost performance in multilingual model performance, but the scalability of these approaches to hundreds of languages or experts tends to be hard to manage. We present Language-specific Matrix Synthesis (LMS), a novel method that addresses the issue. LMS utilizes parameter-efficient and lightweight modules, reducing the number of parameters while outperforming existing methods, e.g., +1.73 BLEU over Switch Transformer on OPUS-100 multilingual translation. Additionally, we introduce Fuse Distillation (FD) to condense multilingual knowledge from multiple LS modules into a single shared module, improving model inference and storage efficiency. Our approach demonstrates superior scalability and performance compared to state-of-the-art methods.",
    abbr={EMNLP},
    selected={true},
}

@inproceedings{tan2024structureaware,
  title={Structure-Aware Path Inference for Neural Finite State Transducers},
  author={Weiting Tan and Chu-Cheng Lin and Jason Eisner},
  booktitle={I Can't Believe It's Not Better Workshop: Failure Modes in the Age of Foundation Models},
  year={2024},
  url={https://openreview.net/forum?id=pTEm4Gz7xL},
  abbr={NeurIPS Workshop}
}


@misc{tan2022bitextmininglowresourcelanguages,
      title={Bitext Mining for Low-Resource Languages via Contrastive Learning}, 
      author={Weiting Tan and Philipp Koehn},
      year={2022},
      eprint={2208.11194},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2208.11194}, 
      abbr={preprint}
}



@inproceedings{tan-etal-2022-doubly,
    title = "Doubly-Trained Adversarial Data Augmentation for Neural Machine Translation",
    author = "Tan, Weiting  and
      Ding, Shuoyang  and
      Khayrallah, Huda  and
      Koehn, Philipp",
    editor = "Duh, Kevin  and
      Guzm{\'a}n, Francisco",
    booktitle = "Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)",
    month = sep,
    year = "2022",
    address = "Orlando, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-research.12",
    pages = "157--174",
    abstract = "Neural Machine Translation (NMT) models are known to suffer from noisy inputs. To make models robust, we generate adversarial augmentation samples that attack the model and preserve the source-side meaning at the same time. To generate such samples, we propose a doubly-trained architecture that pairs two NMT models of opposite translation directions with a joint loss function, which combines the target-side attack and the source-side semantic similarity constraint. The results from our experiments across three different language pairs and two evaluation metrics show that these adversarial samples improve model robustness.",
    abbr={AMTA}
}

